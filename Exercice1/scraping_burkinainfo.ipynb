{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feef2b69",
   "metadata": {},
   "source": [
    "## Exercice 1 : Scraping de données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a5303",
   "metadata": {},
   "source": [
    "### Scraping de données sur le site de Burkina Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb6ddf",
   "metadata": {},
   "source": [
    "# Installation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0174d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/good-boy/CoursLicence3/CoursNLP/projets/WebScraping/TPs/code_tps/env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/good-boy/CoursLicence3/CoursNLP/projets/WebScraping/TPs/code_tps/env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m16.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m16.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m16.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m15.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing-extensions, soupsieve, numpy, idna, charset_normalizer, certifi, requests, pandas, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.5 certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 numpy-2.3.2 pandas-2.3.2 pytz-2025.2 requests-2.32.5 soupsieve-2.8 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "%pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c27042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7942518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des rubriques à scraper\n",
    "rubriques = {\n",
    "   \"politique\": \"https://burkinainfo.com/category/politique/\",\n",
    "    \"economie\": \"https://burkinainfo.com/category/economie/\",\n",
    "    \"societe\" : \"https://burkinainfo.com/category/societe/\",\n",
    "    \"culture\" : \"https://burkinainfo.com/category/culture/\",\n",
    "    \"sport\" : \"https://burkinainfo.com/category/sports/\",\n",
    "    \"tech\" : \"https://burkinainfo.com/category/tie-tech/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5bb27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de scraping\n",
    "def scrap_burkinainfo(pages=5):\n",
    "    articles = []\n",
    "    \n",
    "    # Headers pour éviter les blocages\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    \n",
    "    for rubrique, url in rubriques.items():\n",
    "        print(f\"Scraping de la rubrique: {rubrique}\")\n",
    "        \n",
    "        for page in range(1, pages + 1):\n",
    "            print(f\" Page {page}\")\n",
    "            \n",
    "            # Construction de l'URL pour les pages suivantes\n",
    "            if page == 1:\n",
    "                page_url = url\n",
    "            else:\n",
    "                page_url = f\"{url}page/{page}/\"\n",
    "                \n",
    "            try:\n",
    "                response = requests.get(page_url, headers=headers, timeout=10)\n",
    "                \n",
    "                # Vérification du succès de la requête\n",
    "                if response.status_code != 200:\n",
    "                    print(f\" Erreur {response.status_code} pour la page {page} de {rubrique}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Parsing du contenu HTML\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Debug: afficher la structure HTML pour comprendre\n",
    "                if page == 1 and rubrique == \"politique\":\n",
    "                    print(\"DEBUG: Structure HTML trouvée:\")\n",
    "                    # Chercher différents patterns possibles\n",
    "                    possible_selectors = [\n",
    "                        'article',\n",
    "                        '.post',\n",
    "                        '.entry',\n",
    "                        '[class*=\"post\"]',\n",
    "                        'h1 a, h2 a, h3 a',\n",
    "                        'a[href*=\"burkinainfo.com/20\"]'  # liens d'articles avec date\n",
    "                    ]\n",
    "                    \n",
    "                    for selector in possible_selectors:\n",
    "                        elements = soup.select(selector)\n",
    "                        if elements:\n",
    "                            print(f\"  - Trouvé {len(elements)} éléments avec '{selector}'\")\n",
    "                \n",
    "                # Méthode 1: Chercher tous les liens qui semblent être des articles\n",
    "                article_links = soup.find_all('a', href=re.compile(r'/\\d{4}/\\d{2}/\\d{2}/'))\n",
    "                \n",
    "                print(f\" Trouvé {len(article_links)} liens d'articles potentiels\")\n",
    "                \n",
    "                for link in article_links:\n",
    "                    try:\n",
    "                        href = link.get('href')\n",
    "                        title_text = link.get_text(strip=True)\n",
    "                        \n",
    "                        # Éviter les liens \"Lire la suite\"\n",
    "                        if not title_text or 'Lire la suite' in title_text or len(title_text) < 10:\n",
    "                            continue\n",
    "                            \n",
    "                        # Vérifier que c'est un lien complet\n",
    "                        if href and href.startswith('http'):\n",
    "                            full_link = href\n",
    "                        elif href:\n",
    "                            full_link = urljoin(\"https://burkinainfo.com\", href)\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                        # Éviter les doublons\n",
    "                        if not any(art['link'] == full_link for art in articles):\n",
    "                            articles.append({\n",
    "                                'title': title_text,\n",
    "                                'link': full_link,\n",
    "                                'rubrique': rubrique\n",
    "                            })\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"   Erreur lors du traitement d'un lien: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Méthode 2: Si la méthode 1 ne fonctionne pas, essayer d'autres approches\n",
    "                if not article_links:\n",
    "                    print(\" Méthode 1 échouée, essai de la méthode 2...\")\n",
    "                    \n",
    "                    # Chercher dans des containers plus génériques\n",
    "                    containers = soup.find_all(['div', 'section', 'article'], class_=re.compile(r'post|entry|article|content'))\n",
    "                    \n",
    "                    for container in containers:\n",
    "                        title_elem = container.find(['h1', 'h2', 'h3', 'h4'], recursive=True)\n",
    "                        if title_elem:\n",
    "                            link_elem = title_elem.find('a') or container.find('a')\n",
    "                            if link_elem and link_elem.get('href'):\n",
    "                                href = link_elem.get('href')\n",
    "                                title = title_elem.get_text(strip=True)\n",
    "                                \n",
    "                                if title and len(title) > 10 and 'Lire la suite' not in title:\n",
    "                                    full_link = urljoin(\"https://burkinainfo.com\", href) if not href.startswith('http') else href\n",
    "                                    \n",
    "                                    if not any(art['link'] == full_link for art in articles):\n",
    "                                        articles.append({\n",
    "                                            'title': title,\n",
    "                                            'link': full_link,\n",
    "                                            'rubrique': rubrique\n",
    "                                        })\n",
    "                \n",
    "                print(f\" Articles extraits de cette page: {len([a for a in articles if a['rubrique'] == rubrique]) - len([a for a in articles[:-(len(article_links) if article_links else 0)] if a['rubrique'] == rubrique])}\")\n",
    "                \n",
    "                # Pause entre les requêtes pour éviter d'être bloqué\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except requests.RequestException as e:\n",
    "                print(f\" Erreur de connexion pour la page {page} de {rubrique}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\" Erreur inattendue pour la page {page} de {rubrique}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nTotal des articles récupérés: {len(articles)}\")\n",
    "    \n",
    "    if articles:\n",
    "        # Conversion en DataFrame et sauvegarde\n",
    "        df = pd.DataFrame(articles)\n",
    "        \n",
    "        # Supprimer les doublons basés sur le lien\n",
    "        df = df.drop_duplicates(subset=['link'], keep='first')\n",
    "        \n",
    "        # Créer le dossier s'il n'existe pas\n",
    "        import os\n",
    "        os.makedirs('Docs', exist_ok=True)\n",
    "        \n",
    "        # Sauvegarde\n",
    "        df.to_csv('Docs/burkinainfo_articles.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"Fichier sauvegardé avec {len(df)} articles uniques\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Aucun article trouvé!\")\n",
    "        # Retourner un DataFrame vide avec les bonnes colonnes\n",
    "        return pd.DataFrame(columns=['title', 'link', 'rubrique'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02b1164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping de la rubrique: politique\n",
      " Page 1\n",
      "DEBUG: Structure HTML trouvée:\n",
      "  - Trouvé 10 éléments avec '.post'\n",
      "  - Trouvé 103 éléments avec '[class*=\"post\"]'\n",
      "  - Trouvé 10 éléments avec 'h1 a, h2 a, h3 a'\n",
      "  - Trouvé 108 éléments avec 'a[href*=\"burkinainfo.com/20\"]'\n",
      " Trouvé 46 liens d'articles potentiels\n",
      " Articles extraits de cette page: 18\n",
      " Page 2\n",
      " Trouvé 46 liens d'articles potentiels\n",
      " Articles extraits de cette page: 28\n",
      "Scraping de la rubrique: economie\n",
      " Page 1\n",
      " Trouvé 58 liens d'articles potentiels\n",
      " Articles extraits de cette page: 10\n",
      " Page 2\n",
      " Trouvé 61 liens d'articles potentiels\n",
      " Articles extraits de cette page: 20\n",
      "Scraping de la rubrique: societe\n",
      " Page 1\n",
      " Trouvé 66 liens d'articles potentiels\n",
      " Articles extraits de cette page: 8\n",
      " Page 2\n",
      " Trouvé 66 liens d'articles potentiels\n",
      " Articles extraits de cette page: 18\n",
      "Scraping de la rubrique: culture\n",
      " Page 1\n",
      " Trouvé 46 liens d'articles potentiels\n",
      " Articles extraits de cette page: 10\n",
      " Page 2\n",
      " Trouvé 42 liens d'articles potentiels\n",
      " Articles extraits de cette page: 20\n",
      "Scraping de la rubrique: sport\n",
      " Page 1\n",
      " Trouvé 46 liens d'articles potentiels\n",
      " Articles extraits de cette page: 10\n",
      " Page 2\n",
      " Trouvé 46 liens d'articles potentiels\n",
      " Articles extraits de cette page: 20\n",
      "Scraping de la rubrique: tech\n",
      " Page 1\n",
      " Trouvé 56 liens d'articles potentiels\n",
      " Articles extraits de cette page: 10\n",
      " Page 2\n",
      " Trouvé 56 liens d'articles potentiels\n",
      " Articles extraits de cette page: 20\n",
      "\n",
      "Total des articles récupérés: 126\n",
      "Fichier sauvegardé avec 126 articles uniques\n",
      "Total des articles scrapés: 126\n",
      "\n",
      "Premiers articles:\n",
      "                                               title  \\\n",
      "0  Burkina Faso : Les travailleurs du ministère c...   \n",
      "1  Effort de paix : La diaspora burkinabè et un c...   \n",
      "2  GBAGBO ET THIAM CANDIDATS A LA CANDIDATURE POU...   \n",
      "3  CRISE EN RDC  : Le retour de la paix passera-t...   \n",
      "4  DIOMAYE FAYE A PARIS : Entre question mémoriel...   \n",
      "5  Annonce | Shiloh2025 L’heure de la démarcation...   \n",
      "6  Coopération multilatérale : L’ONU respecte la ...   \n",
      "7  Autorisation de création d’une nouvelle statio...   \n",
      "8  Le Burkina Faso et la Russie discutent du lanc...   \n",
      "9  Faso Mêbo : l’APIM offre 3 bennes de sable à l...   \n",
      "\n",
      "                                                link   rubrique  \n",
      "0  https://burkinainfo.com/2025/08/27/burkina-fas...  politique  \n",
      "1  https://burkinainfo.com/2025/08/27/effort-de-p...  politique  \n",
      "2  https://burkinainfo.com/2025/08/27/gbagbo-et-t...  politique  \n",
      "3  https://burkinainfo.com/2025/08/27/crise-en-rd...  politique  \n",
      "4  https://burkinainfo.com/2025/08/27/diomaye-fay...  politique  \n",
      "5  https://burkinainfo.com/2025/08/27/annonce-shi...  politique  \n",
      "6  https://burkinainfo.com/2025/08/27/cooperation...  politique  \n",
      "7  https://burkinainfo.com/2025/08/27/autorisatio...  politique  \n",
      "8  https://burkinainfo.com/2025/08/27/le-burkina-...  politique  \n",
      "9  https://burkinainfo.com/2025/08/27/faso-mebo-l...  politique  \n"
     ]
    }
   ],
   "source": [
    "# Test de la fonction\n",
    "if __name__ == \"__main__\":\n",
    "    df_articles = scrap_burkinainfo(pages=2)  # Commencer avec 2 pages pour tester\n",
    "    print(f\"Total des articles scrapés: {len(df_articles)}\")\n",
    "    if not df_articles.empty:\n",
    "        print(\"\\nPremiers articles:\")\n",
    "        print(df_articles.head(10))\n",
    "    else:\n",
    "        print(\"Le DataFrame est vide!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
